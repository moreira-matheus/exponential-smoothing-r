---
title: "Exponential Smoothing"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Characteristics of time series data

## 1.1. Time series components

**Trend**:

A trend is a consistent increase or decrease in units of the underlying data.

- Consistent in this context means that there are consecutively increasing peaks during a so-called uptrend and consecutively decreasing lows during a so-called downtrend.

- Also, if there is no trend, this can be called stationary.

The trend component can also sometimes additionally reflect a cycle.

- A cyclic pattern is usually longer than a trend.
- In other words, a trend can be a pattern within a longer cycle. Hence, the trend component is sometimes also called “trend-cycle” component.

<center><img src="./IMG/trend-component.PNG" width=600 alt="Trend component."></center>

**Seasonality**:

Seasonality describes cyclical effects due to the time of the year.

- Daily data may show valleys on the weekend, weekly data may show peaks for the first week of every or some month(s) or monthly data may show lower levels for the winter or summer months respectively.
- It is also possible to have multiple seasonalities.

<center><img src="./IMG/seasonality-component.PNG" width=600 alt="Seasonality component."></center>

**Error or Remainder**:

Error or remainder is the uncategorized component that is part of time series data and is not described by trend and seasonality.

<center><img src="./IMG/error-component.PNG" width=600 alt="Error component."></center>

## 1.2 Time series decomposition

Time series decomposition describes the process of decomposing time series data into its components: (1) Trend, (2) Seasonality and (3) Error.

- It is used to better understand data and builds a basis for some time series forecasting approaches.

The data we will use in this blog post is real-world organic traffic data.

- Before we get to the time series decomposition, we will start with some required prework and plot the data for exploration.
- The first column displays the month of the year and the second column displays the volume of website traffic from organic search (e.g. Google) measured in Google Analytics sessions.

```{r}
data <- read.csv("organic-traffic.csv", header = T)
head(data)
```

The following R code converts the `Month` column in the appropriate format as well as the `Organic Sessions` column into a time series object of monthly data (`frequency = 12`) and plots the data as a line graph.

```{r fig.align="center"}
#Convert "factor" to "Date" according to cell formatting in CSV file
data[,1] <- as.Date(data[,1], format = "%m/%d/%y")

#Convert vector of "Organic Sessions" into time series object
Organic_Traffic <- ts(data[,2], start = c(2014,1), end = c(2018,6), frequency = 12)

#Avoid scientific notation for y-axis values
#options(scipen=999)

#Plot "Organic Traffic" data
plot(Organic_Traffic, main = "Organic Traffic", ylab = "Sessions", ylim = c(0, 700000))
```

The first step for the time series decomposition is to determine whether the underlying seasonality in the time series data is additive or multiplicative.

- Additive seasonality means that the magnitude of the seasonal amplitude swings remains approximately the same throughout independently of the level of the time series.

- Multiplicative seasonality means that the magnitude of the seasonal amplitude changes in proportion to the level of the time series.

The difference can be observed from the two plots in the following image.

- Also, note that the two time series [...] also have an additive and multiplicative trend, which may appear as the more visible feature at first glance.

<center><img src="./IMG/additive-vs-multiplicative-seasonality.PNG" width=600 alt="Additive vs. multiplicative seasonality"></center>

In the case of the “Organic_Traffic” data, the magnitude of seasonality increases proportionally with level suggesting multiplicative seasonality as can be seen in the following image.

<center><img src="./IMG/increasing-magnitude.PNG" width=600 alt="Increasing magnitude of seasonality with level"></center>

Knowing that the seasonality of the “Organic_Traffic” data is multiplicative, we can proceed to the actual time series decomposition.

- There are various different methodologies of time series decomposition including [classical](https://otexts.com/fpp2/classical-decomposition.html), [X11](https://otexts.com/fpp2/x11.html), [SEATS](https://otexts.com/fpp2/seats.html) and [STL](https://otexts.com/fpp2/stl.html) that come with varied advantages and disadvantages as well as requirements for the time unit of the data.

- In this case, we will use the classical time series decomposition as it is widely used while STL (<u>S</u>easonal Decomposition of <u>Time</u> Series by <u>L</u>oess) is more sophisticated.
  - It is executed in R by decompose requiring “additive” or “multiplicative” as input for the type argument, which refers to the seasonal component in the time series.
  
```{r}
#Decompose data into seasonal, trend and irregular components using classical decomposition
fit_decompose <- decompose(Organic_Traffic, type = "multiplicative")
#Print component data of decomposition 
fit_decompose
```

```{r fig.align="center"}
#Plot component data of decomposition
plot(fit_decompose)
```

Since the applied time series decomposition was multiplicative, the individual values for a month can be multiplied to yield the month-respective organic sessions count.

- The mathematical formula is: $y_t = S_t \cdot T_t \cdot R_t$.
  - $S_t$: seasonality component;
  - $T_t$: trend component;
  - $R_t$: remainder (or random).

See below the differences between the real values ($y_t$) and the values reconstruced from the components ($\hat y_t = S_t \cdot T_t \cdot R_t$), for the 2015 data:

```{r fig.align="center"}
seasonal.2015 <- fit_decompose$seasonal[13:24]
trend.2015 <- fit_decompose$trend[13:24]
random.2015 <- fit_decompose$random[13:24]

reconst.2015 <- seasonal.2015 * trend.2015 * random.2015
original.2015 <- fit_decompose$x[13:24]
errors.2015 <- original.2015 - reconst.2015

ymax = max(abs(c(min(errors.2015), max(errors.2015))))

#options(scipen=-3)
plot(errors.2015,
     ylim = c(-2*ymax, 2*ymax), type = "b", col = "black",
     main = "Errors (2015)", xlab = "Month", ylab = expression(y - hat(y)))
abline(a = 0, b = 0, col="darkgray", lty=2)
```

---

> More info on `plotmath` (mathematical annotations on plots) can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/plotmath.html).

---

The steps of the `decompose` command that have led to those calculated values are as follows:

1. <u>Calculate the trend with a centered moving average</u>:

The moving average of order 3, for instance, calculates the average value of the 3 nearest neighboring values of the time series.

- If the moving average is centered, it means that the moving average is only calculated, if the neighboring values are organized symmetrically around the given observation of the time series.
  - In the case of order 3, this means that there is no calculated moving average for the first observation but for the second observation as the latter is centered between the first and third observation.
  - For order 5, the first calculated data point of the centered moving average will only exist for the third observation, which is centered among observations 1 to 5.

- The centering of the moving average causes it to not calculate data points for the beginning and ending of the underlying time series.
  - In our case, given that the first calculated data point of the trend line is in July 2014, 7th observation of the time series, we can conclude that decompose used an order of 13 (6 observations to the left of the centered moving average data point and 6 observations to its right).

2. <u>Remove the trend from the time series</u>:

In the second step, the trend line (based on the centered moving average with order 13 in this case) is removed from the time series.

- This is also called detrending.

- The remaining components in the time series are therefore seasonality and remainder.

3. <u>Calculate the average for each time unit over all time periods</u>:

With the trend removed from the time series, the average for each time unit over all time periods is taken.

- For the monthly “Organic_Traffic” data, this means that the average is calculated for all January observations, February observations and so on over all time periods.
  - All time periods are 4.5 years in this case, which equates 5 observations for January to June and 4 observations for July to December.
  - These average values for each time period are then centered to have a baseline of the time series level without seasonality.
  - In the case of a multiplicative time series, the effect of seasonality for each time period is expressed by a factor.
    - In the case of a multiplicative time series, the effect of seasonality for each time period is expressed by a factor.
  - You can observe that the values of the seasonal component do repeat every 12 months as the seasonality considered exhibits the same annual effect for every year of the dataset.
    - The calculated effect of the seasonality is therefore sensitive to the time window spanned by the dataset. 
    - Longer time spans of data will accordingly yield a better model of the seasonality effect.

You can deseasonalize your traffic using the seasonality factors.

- For December 2014, this looks like this: $\text{Dec}_{2014, \text{deseasonalized}} = \displaystyle\frac{206,609}{0.8500177} = 243,064.4$.
- Accordingly, for November 2014 this looks like this: $\text{Nov}_{2014, \text{deseasonalized}} = \displaystyle\frac{237,982}{0.9979517} = 238,470.5$.
    - If you looked at the “Organic_Traffic” numbers, you will see that December 2014 had $206,609$ organic sessions while November 2014 had $237,982$.
    - So, you would conclude that December must have been an overall weaker month but if you factor out seasonality, December was actually a stronger month than November in terms of number of organic sessions.

```{r fig.align="center"}
len <- length(fit_decompose$x)
deseason.all <- fit_decompose$x / fit_decompose$seasonal

plot(1:len, fit_decompose$x,
     main = "De-seasonalized time series", xlab = "Time steps", ylab = "Num. of sessions",
     lty = 1, lwd = 1.5, type="l", col="black")

lines(1:len, deseason.all,
      lty = 3, lwd=2.0, col="black")

legend("bottomright", lty = c(1,3), lwd = c(1.5, 2.0), col = c("black", "black"), bty = "n",
       legend = c("Original", "De-seasonalized"))
```

4. <u>Calculate the error component</u>:

In the fourth step, the trend and seasonality are removed from the original time series to yield the error component.

```{r fig.align="center"}
plot(1:12, original.2015 / trend.2015,
     main = "Random component (2015)", xlab = "Month", ylab = "Value",
     lty = 1, lwd=1.5, type="l", col="black")

lines(1:12, original.2015 / (trend.2015 * seasonal.2015),
      lty = 3, lwd=2.0, col="black")

legend(x = 1, y = 0.90,
       lty = c(1,3), lwd = c(1.5, 2.0), col = c("black", "black"), bty = "n",
       legend = c(expression("De-trended: " * frac(y[t],T[t]) == S[t] %*% R[t]),
                  expression("De-trended & de-seasonalized: " * frac(y[t],T[t] %*% S[t]) == R[t])))
```

In conclusion, time series decomposition allows you to understand your time series data at a deeper level. For instance, you can quickly identify the underlying trend and take it as the “growth rate” of the organic traffic or you can de-seasonalize data points to identify whether traffic really went down in a given month or whether it was due to seasonality only.

***

# 2. Exponential smoothing for time series forecasts

Exponential smoothing is a popular forecasting method for short-term predictions.

- Such forecasts of future values are based on past data whereby the most recent observations are weighted more than less recent observations.
  - As part of this weighting, constants are being smoothed. This is different from the simple moving average method, in which every data point has equal weight in the average calculation.

- Exponential smoothing introduces the idea of building a forecasted value as the average figure from differently weighted data points for the average calculation.

There are different exponential smoothing methods that differ from each other in the components of the time series that are modeled.

- For instance, simple exponential smoothing (SES) uses only one smoothing constant, double exponential smoothing or Holt exponential smoothing uses two smoothing constants and triple exponential smoothing or Holt-Winters exponential smoothing accordingly uses three smoothing constants.

## 2.1 Simple exponential smoothing

Simple exponential smoothing assumes that the time series data has <u>only a level and some error (or remainder) but no trend or seasonality</u>.

It is therefore not applicable to the “Organic_Traffic” data as that data has underlying seasonality and trend.

- Nevertheless, for illustration purposes, we will apply simple exponential smoothing to our organic traffic data in this section purely to explore its mechanics.

For exponential smoothing, all past observations are part of the calculation for the forecasted value.

- The smoothing parameter $\alpha$ determines the distribution of weights of past observations and with that how heavily a given time period is factored into the forecasted value.

- If the smoothing parameter is close to $1$, recent observations carry more weight and if the smoothing parameter is closer to 0, weights of older and recent observations are more balanced.
  - For example, in the case of $\alpha = 0.5$, the weight halves with every next older observation: $0.5$, $0.25$, $0.125$, $0.0625$, etc.

- As you can see in the following image, the forecasted value of the next period in the future is the sum of the respective products of Weight and past Organic Sessions for every previous time period.

<center><img src="./IMG/simple-exponential-smoothing.PNG" width=600 alt="Simple exponential smoothing."></center>

Here is also the mathematical notation for columns D and E respectively:

$$
\text{Weight}_t = \alpha \cdot (1 - \alpha)^t \\
\text{Forecast} = \displaystyle\sum_{t = 0}^{n} \Big(\text{Organic sessions}_t \cdot \text{Weight}_t\Big)
$$

In this case, our freely chosen smoothing parameter $\alpha = 0.5$ yields a forecast of $591,069$ Organic Sessions for the next future period.

If we fit a simple exponential smoothing model to the “Organic_Traffic” data in R, then the smoothing parameter $\alpha$ is automatically determined by optimizing for minimum forecasting error.

- The following R code fits a simple exponential smoothing model to the “Organic_Traffic” data and prints a summary including the calculated smoothing parameter $\alpha$ and forecasting error measures.
- Note that instead of the `ses(Organic_Traffic)` function, you can also use `ets(Organic_Traffic, model = "ANN")` from the `forecast` package but in this section we go with the simpler `ses` function.

```{r warning=FALSE,message=FALSE}
#Install and load forecast package if not installed
if(!require(forecast)) {
  install.packages("forecast", dependencies = T, repos = "http://cran.us.r-project.org")
  library(forecast) }

#Fit simple exponential smoothing model to data and show summary
fit_ses <- ses(Organic_Traffic)
summary(fit_ses)
```

> More on the `ses()` function [here](https://www.rdocumentation.org/packages/forecast/versions/8.10/topics/ses).

You can see in this output under Smoothing parameters that $\alpha = 0.6347$.

- This means that $63.47\%$ of the forecast are based on the most recent observation.

Below that various Error measures - that will be discussed later - are shown and the summary ends with Forecasts.

Since simple exponential smoothing assumes there is no trend or seasonality the forecasts for future time periods is at a certain level but flat in its development over time.

- `Lo 80` and `Hi 80` are the boundaries of an $80\%$ confidence interval and accordingly, the wider bounds of `Lo 95` and `Hi 95` represent the $95\%$ confidence interval (the higher the confidence, the wider the bounds).

- Also, note that with increasing distance into the future the confidence intervals widen.

The plot function produces the `plot` below which represents the forecasted level for future time periods and visualizes the $80\%$ and $95\%$ confidence intervals.

```{r fig.align="center"}
#Plot the forecasted values
plot(fit_ses)
```

**Component form**: 

$$
\begin{cases}
\text{(Forecast eq.)} & \quad \hat y_{t+h|t} = \mathcal{l}_t \\
\text{(Level eq.)} & \quad \mathcal{l}_t = \alpha y_t + (1-\alpha) \mathcal{l}_{t-1}
\end{cases}
$$

- Source [here](https://otexts.com/fpp2/ses.html).

## 2.2 Holt exponential smoothing

Holt exponential smoothing is a time series forecasting approach that <u>fits time series data with an overall level as well as a trend</u>.

- Additionally to simple exponential smoothing, which uses smoothing parameter $\alpha$ only there is also a $\beta$ smoothing parameter for the exponential decay of the modeled trend component. 
  - This $\beta$ smoothing parameter ranges between $0$ and $1$, with higher values indicating more weight to recent observations.
  - A $\beta$ value of $0.5$ means that the most recent observation’s trend component is weighted with $50\%$ in the modeled trend slope.

- The mechanics of the $\beta$ smoothing parameter are the same as described for the $\alpha$ smoothing parameter previously. The difference is that they refer to different time series components, e.g. trend and level, respectively.

```{r}
#Fit Holt exponential smoothing model to data and show summary
fit_holt <- holt(Organic_Traffic)
summary(fit_holt)
```

> More on the `holt()` function [here](https://www.rdocumentation.org/packages/forecast/versions/8.10/topics/ses).

The output of the `summary` function will this time detect values for both the $\alpha$ and $\beta$ smoothing parameters by optimizing for minimum forecasting error.

- The forecasted values take trend into account and hence, you can see the increase in the values over time.

The `plot` function produces the plot below based on the forecasted values and the confidence intervals.

```{r fig.align="center"}
#Plot the forecasted values
plot(fit_holt)
```

The inclusion of the trend component in the model produces a forecast that takes the uptrend of the “Organic_Traffic” data accordingly into account. However, the seasonal fluctuations in the data are not part of the model.

**Component form**: 

$$
\begin{cases}
\text{(Forecast eq.)} & \quad \hat y_{t+h|t} = \mathcal{l}_t + h b_t\\
\text{(Level eq.)} & \quad \mathcal{l}_t = \alpha y_t + (1-\alpha) (\mathcal{l}_{t-1} + b_{t-1}) \\
\text{(Trend eq.)} & \quad b_t = \beta (l_t - l_{t-1}) + (1 - \beta) b_{t-1}
\end{cases}
$$

- Source [here](https://otexts.com/fpp2/holt.html).

## 2.3 Holt-Winters exponential smoothing

Holt-Winters exponential smoothing is a time series forecasting approach that <u>takes the overall level, trend and seasonality of the underlying dataset into account for its forecast</u>.

- Hence, the Holt-Winters model has three smoothing parameters indicating the exponential decay from most recent to older observations: $\alpha$ for the level component, $\beta$ for the trend component, and $\gamma$ for the seasonality component.

The following R code fits the Holt-Winters model to the time series data.

- The argument `h = 10` sets the forecasted time periods to $10$ (its default is $24$), while the default for the previously used functions ses and holt is $10$.

- Also, the argument `seasonal = "multiplicative"` indicates that the seasonal component in the data increases in proportion to the level.

```{r}
#Fit Holt-Winters exponential smoothing model to data and show summary
fit_hw <- hw(Organic_Traffic, h = 10, seasonal = "multiplicative")
summary(fit_hw)
```

> More on the `hw()` function [here](https://www.rdocumentation.org/packages/forecast/versions/8.10/topics/ses).

The `summary` function shows the estimates for $\alpha$, $\beta$, and also $\gamma$ for the seasonality component.

The `plot` function visualizes the forecast including seasonality.

```{r fig.align="center"}
#Plot the forecasted values
plot(fit_hw)
```

**Component form**: 

$$
\begin{cases}
\text{(Forecast eq.)} & \quad \hat y_{t+h|t} = \mathcal{l}_t + h b_t + s_{t+h -m (k+1)} \\
\text{(Level eq.)} & \quad \mathcal{l}_t = \alpha (y_t - s_{t-m}) + (1-\alpha) (\mathcal{l}_{t-1} + b_{t-1}) \\
\text{(Trend eq.)} & \quad b_t = \beta (l_t - l_{t-1}) + (1 - \beta) b_{t-1} \\
\text{(Seasonality eq.)} & \quad s_t = \gamma (y_t - l_{t-1} - b_{t-1}) + (1-\gamma) s_{t-m}
\end{cases}
$$

- Source [here](https://otexts.com/fpp2/holt-winters.html).

## 2.4 Automated exponential smoothing forecasts

The previous three sections introduced simple, Holt and Holt-Winters exponential smoothing. These three exponential smoothing models use different (combinations of) time series components (e.g. level, trend, seasonality) for their respective forecasts.

- <u>There is also an automated exponential smoothing forecast that can be accomplished with the `ets()` function from the `forecast` package</u>.

  - In this case, it is automatically determined whether a time series component is additive or multiplicative.
  - The initial state variable and smoothing parameters [...] are optimized to yield the model by default with the least [AICc](https://en.wikipedia.org/wiki/Akaike_information_criterion#AICc) (Akaike information criterion with a correction for small sample sizes) as chosen automated forecast model.

- The previously used functions `ses()`, `holt()` and `hw()` are convenience wrappers to the `ets()` function.

- The `ets()` function takes several arguments, including `model`, which takes three letters as input.
  - The first letter denotes the error type (`A`, `M` or `Z`), the second letter denotes the trend type (`N`, `A`, `M` or `Z`), and the third letter denotes the seasonality type (`N`, `A`, `M` or `Z`).
  - The letters stand for `N` = none, `A` = additive, `M` = multiplicative and `Z` = automatically selected.
  - This means that `ses(Organic_Traffic)` is, for instance, the convenience wrapper to `ets(Organic_Traffic, model = "ANN")`, which fits a model with additive error and without trend or seasonality.

- Also, the `ets()` function can be used with `Z` for all time series components or without the model argument to fit the automated exponential smoothing forecast to the data.
  - This means `ets(Organic_Traffic)` is the same like `ets(Organic_Traffic, model = "ZZZ")`.
  - One drawback of the `ets()` function is that no time horizon can be chosen.

```{r}
#Fit automated exponential smoothing model to data and show summary
fit_auto <- forecast(Organic_Traffic, h = 10)
summary(fit_auto)
```

From the output in the previous image, you can see an AICc value of $1338.090$, which is lower than for all other presented exponential smoothing models.

- Also, the headline of the plot below contains `ETS(M,A,M)` showing that the automated model characterized error of the “Organic_Traffic” data as multiplicative (`M`), trend as additive (`A`) and seasonality as multiplicative (`M`).

```{r fig.align="center"}
#Plot the forecasted values
plot(forecast(fit_auto))
```

